{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from tensorflow.contrib import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conv layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# image generated\n",
    "height, width = 3, 3\n",
    "images = np.random.uniform(size=(5, height, width, 3))\n",
    "output = tf.contrib.layers.avg_pool2d(images, [3, 3])\n",
    "\n",
    "# conv layer\n",
    "output = tf.contrib.layers.convolution2d(images, num_outputs=32, kernel_size=[3, 3])\n",
    "weights = tf.contrib.framework.get_variables_by_name('weights')[0]\n",
    "weights_shape = weights.get_shape().as_list()\n",
    "\n",
    "# image matrix\n",
    "images = tf.random_uniform((5, height, width, 32), seed=1)\n",
    "\n",
    "# arg_scope\n",
    "with tf.contrib.framework.arg_scope([tf.contrib.layers.convolution2d],\n",
    "                                    normalizer_fn=tf.contrib.layers.batch_norm,\n",
    "                                    normalizer_params={'decay': 0.9}):\n",
    "    net = tf.contrib.layers.convolution2d(images, 32, [3 ,3])\n",
    "    net = tf.contrib.layers.convolution2d(net, 32, [3, 3])\n",
    "print(len(tf.contrib.framework.get_variables('Conv/BatchNorm')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fully_connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"fe/fc/Relu:0\", shape=(5, 3, 3, 7), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "height, width = 3, 3\n",
    "inputs = np.random.uniform(size=(5, height, width, 3))\n",
    "with tf.name_scope('fe'):\n",
    "    fc = tf.contrib.layers.fully_connected(inputs, 7,\n",
    "                outputs_collections='outputs', scope='fc') \n",
    "output_collected = tf.get_collection('outputs')[0]\n",
    "print(output_collected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-ce9b05a02c31>:5: absolute_difference (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.absolute_difference instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:299: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n",
      "Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-4-ce9b05a02c31>:11: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n",
      "Tensor(\"softmax_cross_entropy_loss/value:0\", shape=(), dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-4-ce9b05a02c31>:19: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n",
      "Tensor(\"softmax_cross_entropy_loss_1/value:0\", shape=(), dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-4-ce9b05a02c31>:25: sparse_softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.sparse_softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:434: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n",
      "Tensor(\"sparse_softmax_cross_entropy_loss/value:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# absolute difference\n",
    "predictions = tf.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n",
    "targets = tf.constant([1, 9, 2, -5, -2, 6], shape=(2 ,3))\n",
    "weight = tf.constant([1.2, 0.0])\n",
    "loss = tf.contrib.losses.absolute_difference(predictions, targets, weight) \n",
    "print(tf.reduce_mean(loss))\n",
    "\n",
    "# softmax cross entropy\n",
    "predictions = tf.constant([[10.0, 0.0, 0.0], [0.0, 10.0, 0.0], [0.0, 0.0, 10.0]])\n",
    "labels = tf.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "loss = tf.contrib.losses.softmax_cross_entropy(predictions, labels)\n",
    "print(loss)\n",
    "\n",
    "# softmax with label_smoothing\n",
    "logits = tf.constant([[100.0, -100.0, -100.0]])\n",
    "labels = tf.constant(([[1, 0, 0]]))\n",
    "label_smoothing = 0.1\n",
    "loss = tf.contrib.losses.softmax_cross_entropy(logits, labels,\n",
    "                        label_smoothing=label_smoothing)\n",
    "print(loss)\n",
    "\n",
    "# sparse softmax cross entropy\n",
    "logits = tf.constant([[10.0, 0.0, 0.0], [0.0, 10.0, 0.0],[0.0, 0.0, 10.0]])\n",
    "labels = tf.constant([[0], [1], [2]], dtype=tf.int64)\n",
    "loss = tf.contrib.losses.sparse_softmax_cross_entropy(logits, labels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_RealValuedColumn(column_name='age', dimension=1, default_value=None, dtype=tf.float32, normalizer=None)\n",
      "_SparseColumn(column_name='gender', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('female', 'male'), num_oov_buckets=0, vocab_size=2, default_value=-1), combiner='sum', dtype=tf.string)\n",
      "_SparseColumn(column_name='education', is_integerized=False, bucket_size=1000, lookup_config=None, combiner='sum', dtype=tf.string)\n",
      "_BucketizedColumn(source_column=_RealValuedColumn(column_name='age', dimension=1, default_value=None, dtype=tf.float32, normalizer=None), boundaries=(19, 20, 30, 35, 40, 45, 55, 58, 60))\n"
     ]
    }
   ],
   "source": [
    "# define data\n",
    "age = layers.real_valued_column(\"age\")\n",
    "income = layers.real_valued_column(\"income\")\n",
    "spending = layers.real_valued_column(\"spending\")\n",
    "hours_of_work = layers.real_valued_column(\"hours_of_work\")\n",
    "\n",
    "gender = layers.sparse_column_with_keys(column_name=\"gender\", keys=[\"female\", \"male\"])\n",
    "education = layers.sparse_column_with_hash_bucket(\"education\", hash_bucket_size=1000)\n",
    "age_range = layers.bucketized_column(age, boundaries=[19, 20, 30, 35, 40, 45, 55, 58, 60])\n",
    "\n",
    "print(age)\n",
    "print(gender)\n",
    "print(education)\n",
    "print(age_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:create_partitioned_variables is deprecated.  Use tf.get_variable with a partitioner set, or tf.get_partitioned_variable_list, instead.\n"
     ]
    }
   ],
   "source": [
    "indices = [[0, 0], [0, 1], [0, 2], [1, 0], [3, 0], [4, 0], [4, 1]]\n",
    "ids = [0, 1, -1, -1, 2, 0, 1]\n",
    "weights = [1.0, 2.0, 1.0, 1.0, 3.0, 0.0, -0.5]\n",
    "shape = [5, 4]\n",
    "\n",
    "sparse_ids = tf.SparseTensor(tf.constant(indices, tf.int64),\n",
    "                            tf.constant(ids, tf.int64),\n",
    "                            tf.constant(shape, tf.int64))\n",
    "\n",
    "sparse_weights = tf.SparseTensor(tf.constant(indices, tf.int64),\n",
    "                                tf.constant(weights, tf.float32),\n",
    "                                tf.constant(shape, tf.int64))\n",
    "\n",
    "vocab_size = 4\n",
    "embed_dim = 4\n",
    "num_shards = 1\n",
    "\n",
    "embedding_weights = tf.create_partitioned_variables(\n",
    "    shape=[vocab_size, embed_dim],\n",
    "    slicing=[num_shards, 1],\n",
    "    initializer=tf.truncated_normal_initializer(mean=0.0,\n",
    "                stddev=1.0/math.sqrt(vocab_size), dtype=tf.float32))\n",
    "\n",
    "sess = tf.Session()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
