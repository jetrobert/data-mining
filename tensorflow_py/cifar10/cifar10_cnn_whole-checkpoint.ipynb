{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from six.moves import urllib\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define weight function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_with_weight_loss(shape, stddev, wl):\n",
    "    var = tf.Variable(tf.truncated_normal(shape, stddev=stddev))\n",
    "    if wl is not None:\n",
    "        weight_loss = tf.multiply(tf.nn.l2_loss(var), wl, name='weight_loss')\n",
    "        tf.add_to_collection('losses', weight_loss)\n",
    "    return var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# download function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maybe_download_and_extract(data_dir, data_url):\n",
    "  dest_directory = data_dir\n",
    "  if not os.path.exists(dest_directory):\n",
    "    os.makedirs(dest_directory)\n",
    "  filename = data_url.split('/')[-1]\n",
    "  filepath = os.path.join(dest_directory, filename)\n",
    "  if not os.path.exists(filepath):\n",
    "    def _progress(count, block_size, total_size):\n",
    "      sys.stdout.write('\\r>> Downloading %s %.1f%%' % (filename,\n",
    "          float(count * block_size) / float(total_size) * 100.0))\n",
    "      sys.stdout.flush()\n",
    "    filepath, _ = urllib.request.urlretrieve(data_url, filepath, _progress)\n",
    "    print()\n",
    "    statinfo = os.stat(filepath)\n",
    "    print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
    "  extracted_dir_path = os.path.join(dest_directory, 'cifar-10-batches-bin')\n",
    "  if not os.path.exists(extracted_dir_path):\n",
    "    tarfile.open(filepath, 'r:gz').extractall(dest_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/tmp/cifar10_data/cifar-10-batches-bin'\n",
    "data_url = 'http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'\n",
    "maybe_download_and_extract(data_dir, data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reads and parses examples from data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_cifar10(filename_queue):\n",
    "  class CIFAR10Record(object):\n",
    "    pass\n",
    "  result = CIFAR10Record()\n",
    "\n",
    "  # Dimensions of the images in the CIFAR-10 dataset.\n",
    "  label_bytes = 1  # 2 for CIFAR-100\n",
    "  result.height = 32\n",
    "  result.width = 32\n",
    "  result.depth = 3\n",
    "  image_bytes = result.height * result.width * result.depth\n",
    "  # Every record consists of a label followed by the image, with a\n",
    "  # fixed number of bytes for each.\n",
    "  record_bytes = label_bytes + image_bytes\n",
    "\n",
    "  # Read a record, getting filenames from the filename_queue.  No\n",
    "  # header or footer in the CIFAR-10 format, so we leave header_bytes\n",
    "  # and footer_bytes at their default of 0.\n",
    "  reader = tf.FixedLengthRecordReader(record_bytes=record_bytes)\n",
    "  result.key, value = reader.read(filename_queue)\n",
    "\n",
    "  # Convert from a string to a vector of uint8 that is record_bytes long.\n",
    "  record_bytes = tf.decode_raw(value, tf.uint8)\n",
    "\n",
    "  # The first bytes represent the label, which we convert from uint8->int32.\n",
    "  result.label = tf.cast(\n",
    "      tf.strided_slice(record_bytes, [0], [label_bytes]), tf.int32)\n",
    "\n",
    "  # The remaining bytes after the label represent the image, which we reshape\n",
    "  # from [depth * height * width] to [depth, height, width].\n",
    "  depth_major = tf.reshape(\n",
    "      tf.strided_slice(record_bytes, [label_bytes],\n",
    "                       [label_bytes + image_bytes]),\n",
    "      [result.depth, result.height, result.width])\n",
    "  # Convert from [depth, height, width] to [height, width, depth].\n",
    "  result.uint8image = tf.transpose(depth_major, [1, 2, 0])\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct a queued batch of images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _generate_image_and_label_batch(image, label, min_queue_examples,\n",
    "                                     batch_size, shuffle):\n",
    "  # Create a queue that shuffles the examples, and then\n",
    "  # read 'batch_size' images + labels from the example queue.\n",
    "  num_preprocess_threads = 16\n",
    "  if shuffle:\n",
    "    images, label_batch = tf.train.shuffle_batch(\n",
    "        [image, label],\n",
    "        batch_size=batch_size,\n",
    "        num_threads=num_preprocess_threads,\n",
    "        capacity=min_queue_examples + 3 * batch_size,\n",
    "        min_after_dequeue=min_queue_examples)\n",
    "  else:\n",
    "    images, label_batch = tf.train.batch(\n",
    "        [image, label],\n",
    "        batch_size=batch_size,\n",
    "        num_threads=num_preprocess_threads,\n",
    "        capacity=min_queue_examples + 3 * batch_size)\n",
    "\n",
    "  # Display the training images in the visualizer.\n",
    "  tf.summary.image('images', images)\n",
    "\n",
    "  return images, tf.reshape(label_batch, [batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# distorted input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distorted_inputs(data_dir, batch_size, image_size, num_examples_per_epoch_for_train):\n",
    "  filenames = [os.path.join(data_dir, 'data_batch_%d.bin' % i)\n",
    "               for i in xrange(1, 6)]\n",
    "  for f in filenames:\n",
    "    if not tf.gfile.Exists(f):\n",
    "      raise ValueError('Failed to find file: ' + f)\n",
    "\n",
    "  # Create a queue that produces the filenames to read.\n",
    "  filename_queue = tf.train.string_input_producer(filenames)\n",
    "\n",
    "  # Read examples from files in the filename queue.\n",
    "  read_input = read_cifar10(filename_queue)\n",
    "  reshaped_image = tf.cast(read_input.uint8image, tf.float32)\n",
    "\n",
    "  height = image_size\n",
    "  width = image_size\n",
    "\n",
    "  # Image processing for training the network. Note the many random\n",
    "  # distortions applied to the image.\n",
    "\n",
    "  # Randomly crop a [height, width] section of the image.\n",
    "  distorted_image = tf.random_crop(reshaped_image, [height, width, 3])\n",
    "\n",
    "  # Randomly flip the image horizontally.\n",
    "  distorted_image = tf.image.random_flip_left_right(distorted_image)\n",
    "\n",
    "  # Because these operations are not commutative, consider randomizing\n",
    "  # the order their operation.\n",
    "  # NOTE: since per_image_standardization zeros the mean and makes\n",
    "  # the stddev unit, this likely has no effect see tensorflow#1458.\n",
    "  distorted_image = tf.image.random_brightness(distorted_image,\n",
    "                                               max_delta=63)\n",
    "  distorted_image = tf.image.random_contrast(distorted_image,\n",
    "                                             lower=0.2, upper=1.8)\n",
    "\n",
    "  # Subtract off the mean and divide by the variance of the pixels.\n",
    "  float_image = tf.image.per_image_standardization(distorted_image)\n",
    "\n",
    "  # Set the shapes of tensors.\n",
    "  float_image.set_shape([height, width, 3])\n",
    "  read_input.label.set_shape([1])\n",
    "\n",
    "  # Ensure that the random shuffling has good mixing properties.\n",
    "  min_fraction_of_examples_in_queue = 0.4\n",
    "  min_queue_examples = int(num_examples_per_epoch_for_train *\n",
    "                           min_fraction_of_examples_in_queue)\n",
    "  print ('Filling queue with %d CIFAR images before starting to train. '\n",
    "         'This will take a few minutes.' % min_queue_examples)\n",
    "\n",
    "  # Generate a batch of images and labels by building up a queue of examples.\n",
    "  return _generate_image_and_label_batch(float_image, read_input.label,\n",
    "                                         min_queue_examples, batch_size,\n",
    "                                         shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct input for evaluation using the Reader ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inputs(eval_data, data_dir, batch_size, image_size,  \n",
    "           num_examples_per_epoch_for_train, num_examples_per_epoch_for_eval):\n",
    "  if not eval_data:\n",
    "    filenames = [os.path.join(data_dir, 'data_batch_%d.bin' % i)\n",
    "                 for i in xrange(1, 6)]\n",
    "    num_examples_per_epoch = num_examples_per_epoch_for_train\n",
    "  else:\n",
    "    filenames = [os.path.join(data_dir, 'test_batch.bin')]\n",
    "    num_examples_per_epoch = num_examples_per_epoch_for_eval\n",
    "\n",
    "  for f in filenames:\n",
    "    if not tf.gfile.Exists(f):\n",
    "      raise ValueError('Failed to find file: ' + f)\n",
    "\n",
    "  # Create a queue that produces the filenames to read.\n",
    "  filename_queue = tf.train.string_input_producer(filenames)\n",
    "\n",
    "  # Read examples from files in the filename queue.\n",
    "  read_input = read_cifar10(filename_queue)\n",
    "  reshaped_image = tf.cast(read_input.uint8image, tf.float32)\n",
    "\n",
    "  height = image_size\n",
    "  width = image_size\n",
    "\n",
    "  # Image processing for evaluation.\n",
    "  # Crop the central [height, width] of the image.\n",
    "  resized_image = tf.image.resize_image_with_crop_or_pad(reshaped_image,\n",
    "                                                         height, width)\n",
    "\n",
    "  # Subtract off the mean and divide by the variance of the pixels.\n",
    "  float_image = tf.image.per_image_standardization(resized_image)\n",
    "\n",
    "  # Set the shapes of tensors.\n",
    "  float_image.set_shape([height, width, 3])\n",
    "  read_input.label.set_shape([1])\n",
    "\n",
    "  # Ensure that the random shuffling has good mixing properties.\n",
    "  min_fraction_of_examples_in_queue = 0.4\n",
    "  min_queue_examples = int(num_examples_per_epoch *\n",
    "                           min_fraction_of_examples_in_queue)\n",
    "\n",
    "  # Generate a batch of images and labels by building up a queue of examples.\n",
    "  return _generate_image_and_label_batch(float_image, read_input.label,\n",
    "                                         min_queue_examples, batch_size,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128 \n",
    "image_size = 24\n",
    "num_examples_per_epoch_for_train = 50000\n",
    "num_examples_per_epoch_for_eval = 10000\n",
    "\n",
    "# 16 independent threads are used to speed tasks for distorted_in|puts\n",
    "images_train, labels_train = distorted_inputs(\n",
    "    data_dir=data_dir, batch_size=batch_size, image_size=image_size,\n",
    "    num_examples_per_epoch_for_train=num_examples_per_epoch_for_train)\n",
    "\n",
    "images_test, labels_test = inputs(\n",
    "    eval_data=True, data_dir=data_dir, batch_size=batch_size, image_size=image_size, \n",
    "    num_examples_per_epoch_for_train=num_examples_per_epoch_for_train,\n",
    "    num_examples_per_epoch_for_eval=num_examples_per_epoch_for_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_holder = tf.placeholder(tf.float32, [batch_size, 24, 24, 3])\n",
    "label_holder = tf.placeholder(tf.int32, [batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build network framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1st layer conv\n",
    "weight1 = variable_with_weight_loss(shape=[5, 5, 3, 64], stddev=5e-2, wl=0.0)\n",
    "kernel1 = tf.nn.conv2d(image_holder, weight1, [1, 1, 1, 1], padding='SAME')\n",
    "bias1 = tf.Variable(tf.constant(0.0, shape=[64]))\n",
    "conv1 = tf.nn.relu(tf.nn.bias_add(kernel1, bias1))\n",
    "pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001/9.0, beta=0.75)\n",
    "\n",
    "# 2nd layer conv\n",
    "weight2 = variable_with_weight_loss(shape=[5, 5, 64, 64], stddev=5e-2, wl=0.0)\n",
    "kernel2= tf.nn.conv2d(norm1, weight2, [1, 1, 1, 1], padding='SAME')\n",
    "bias2 = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "conv2 = tf.nn.relu(tf.nn.bias_add(kernel2, bias2))\n",
    "norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001/9.0, beta=0.75)\n",
    "pool2 = tf.nn.max_pool(norm2, ksize=[1 ,3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# 1st fully connected layer\n",
    "reshape = tf.reshape(pool2, [batch_size, -1])\n",
    "dim = reshape.get_shape()[1].value\n",
    "weight3 = variable_with_weight_loss(shape=[dim, 384], stddev=0.04, wl=0.004)\n",
    "bias3 = tf.Variable(tf.constant(0.1, shape=[384]))\n",
    "local3 = tf.nn.relu(tf.matmul(reshape, weight3) + bias3)\n",
    "\n",
    "# 2nd fully connected layer\n",
    "weight4 = variable_with_weight_loss(shape=[384, 192], stddev=0.04, wl=0.004)\n",
    "bias4 = tf.Variable(tf.constant(0.1, shape=[192]))\n",
    "local4 = tf.nn.relu(tf.matmul(local3, weight4) + bias4)\n",
    "\n",
    "# 3rd fully connected layer\n",
    "weight5 = variable_with_weight_loss(shape=[192, 10], stddev=1/192.0, wl=0.0)\n",
    "bias5 = tf.Variable(tf.constant(0.0, shape=[10]))\n",
    "# model inference output\n",
    "logits = tf.add(tf.matmul(local4, weight5), bias5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(logits, labels):\n",
    "    labels = tf.cast(labels, tf.int64)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels, name='cross_entropy_per_example')\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "    tf.add_to_collection('losses', cross_entropy_mean)\n",
    "    return tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "\n",
    "loss = loss(logits, label_holder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimization and top accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_op = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
    "top_k_op = tf.nn.in_top_k(logits, label_holder, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start data augmentation threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Thread(Thread-4, started daemon 140442725250816)>,\n",
       " <Thread(Thread-5, started daemon 140442716858112)>,\n",
       " <Thread(Thread-6, started daemon 140442704279296)>,\n",
       " <Thread(Thread-7, started daemon 140442695886592)>,\n",
       " <Thread(Thread-8, started daemon 140442410678016)>,\n",
       " <Thread(Thread-9, started daemon 140442402285312)>,\n",
       " <Thread(Thread-10, started daemon 140442393892608)>,\n",
       " <Thread(Thread-11, started daemon 140442385499904)>,\n",
       " <Thread(Thread-12, started daemon 140442377107200)>,\n",
       " <Thread(Thread-13, started daemon 140442368714496)>,\n",
       " <Thread(Thread-14, started daemon 140442360321792)>,\n",
       " <Thread(Thread-15, started daemon 140441806698240)>,\n",
       " <Thread(Thread-16, started daemon 140441798305536)>,\n",
       " <Thread(Thread-17, started daemon 140441789912832)>,\n",
       " <Thread(Thread-18, started daemon 140441781520128)>,\n",
       " <Thread(Thread-19, started daemon 140441773127424)>,\n",
       " <Thread(Thread-20, started daemon 140441764734720)>,\n",
       " <Thread(Thread-21, started daemon 140441756342016)>,\n",
       " <Thread(Thread-22, started daemon 140441538262784)>,\n",
       " <Thread(Thread-23, started daemon 140441529870080)>,\n",
       " <Thread(Thread-24, started daemon 140441521477376)>,\n",
       " <Thread(Thread-25, started daemon 140441513084672)>,\n",
       " <Thread(Thread-26, started daemon 140441504691968)>,\n",
       " <Thread(Thread-27, started daemon 140441496299264)>,\n",
       " <Thread(Thread-28, started daemon 140441487906560)>,\n",
       " <Thread(Thread-29, started daemon 140441479513856)>,\n",
       " <Thread(Thread-30, started daemon 140441471121152)>,\n",
       " <Thread(Thread-31, started daemon 140441462728448)>,\n",
       " <Thread(Thread-32, started daemon 140441454335744)>,\n",
       " <Thread(Thread-33, started daemon 140441445943040)>,\n",
       " <Thread(Thread-34, started daemon 140441437550336)>,\n",
       " <Thread(Thread-35, started daemon 140441429157632)>,\n",
       " <Thread(Thread-36, started daemon 140441420764928)>,\n",
       " <Thread(Thread-37, started daemon 140441412372224)>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.start_queue_runners()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss=4.67 (120.9 examples/sec, 1.059 sec/batch)\n",
      "step 10, loss=3.73 (200.5 examples/sec, 0.638 sec/batch)\n",
      "step 20, loss=3.15 (122.5 examples/sec, 1.045 sec/batch)\n",
      "step 30, loss=2.69 (170.6 examples/sec, 0.750 sec/batch)\n",
      "step 40, loss=2.60 (188.7 examples/sec, 0.678 sec/batch)\n",
      "step 50, loss=2.30 (186.9 examples/sec, 0.685 sec/batch)\n",
      "step 60, loss=2.40 (187.4 examples/sec, 0.683 sec/batch)\n",
      "step 70, loss=1.98 (186.5 examples/sec, 0.686 sec/batch)\n",
      "step 80, loss=2.01 (188.9 examples/sec, 0.678 sec/batch)\n",
      "step 90, loss=1.96 (171.2 examples/sec, 0.748 sec/batch)\n",
      "step 100, loss=1.88 (170.3 examples/sec, 0.752 sec/batch)\n",
      "step 110, loss=2.03 (171.6 examples/sec, 0.746 sec/batch)\n",
      "step 120, loss=1.73 (178.6 examples/sec, 0.717 sec/batch)\n",
      "step 130, loss=1.96 (189.3 examples/sec, 0.676 sec/batch)\n",
      "step 140, loss=1.95 (189.1 examples/sec, 0.677 sec/batch)\n",
      "step 150, loss=1.79 (99.7 examples/sec, 1.284 sec/batch)\n",
      "step 160, loss=1.78 (180.5 examples/sec, 0.709 sec/batch)\n",
      "step 170, loss=1.73 (171.6 examples/sec, 0.746 sec/batch)\n",
      "step 180, loss=1.82 (165.1 examples/sec, 0.775 sec/batch)\n",
      "step 190, loss=1.85 (124.9 examples/sec, 1.025 sec/batch)\n",
      "step 200, loss=1.68 (77.4 examples/sec, 1.653 sec/batch)\n",
      "step 210, loss=1.65 (116.6 examples/sec, 1.098 sec/batch)\n",
      "step 220, loss=1.73 (114.0 examples/sec, 1.123 sec/batch)\n",
      "step 230, loss=1.76 (172.8 examples/sec, 0.741 sec/batch)\n",
      "step 240, loss=1.66 (94.4 examples/sec, 1.355 sec/batch)\n",
      "step 250, loss=1.75 (174.0 examples/sec, 0.736 sec/batch)\n",
      "step 260, loss=1.65 (170.2 examples/sec, 0.752 sec/batch)\n",
      "step 270, loss=1.64 (166.7 examples/sec, 0.768 sec/batch)\n",
      "step 280, loss=1.54 (171.0 examples/sec, 0.748 sec/batch)\n",
      "step 290, loss=1.66 (119.7 examples/sec, 1.069 sec/batch)\n",
      "step 300, loss=1.60 (123.7 examples/sec, 1.035 sec/batch)\n",
      "step 310, loss=1.65 (188.9 examples/sec, 0.678 sec/batch)\n",
      "step 320, loss=1.62 (189.4 examples/sec, 0.676 sec/batch)\n",
      "step 330, loss=1.49 (101.3 examples/sec, 1.263 sec/batch)\n",
      "step 340, loss=1.48 (176.2 examples/sec, 0.726 sec/batch)\n",
      "step 350, loss=1.71 (172.0 examples/sec, 0.744 sec/batch)\n",
      "step 360, loss=1.52 (172.0 examples/sec, 0.744 sec/batch)\n",
      "step 370, loss=1.41 (151.0 examples/sec, 0.848 sec/batch)\n",
      "step 380, loss=1.60 (190.0 examples/sec, 0.674 sec/batch)\n",
      "step 390, loss=1.45 (190.9 examples/sec, 0.671 sec/batch)\n",
      "step 400, loss=1.47 (101.1 examples/sec, 1.266 sec/batch)\n",
      "step 410, loss=1.54 (181.8 examples/sec, 0.704 sec/batch)\n",
      "step 420, loss=1.46 (171.2 examples/sec, 0.747 sec/batch)\n",
      "step 430, loss=1.72 (170.7 examples/sec, 0.750 sec/batch)\n",
      "step 440, loss=1.48 (173.6 examples/sec, 0.737 sec/batch)\n",
      "step 450, loss=1.32 (120.7 examples/sec, 1.060 sec/batch)\n",
      "step 460, loss=1.49 (189.6 examples/sec, 0.675 sec/batch)\n",
      "step 470, loss=1.71 (190.3 examples/sec, 0.673 sec/batch)\n",
      "step 480, loss=1.53 (101.0 examples/sec, 1.268 sec/batch)\n",
      "step 490, loss=1.44 (171.8 examples/sec, 0.745 sec/batch)\n",
      "step 500, loss=1.62 (172.6 examples/sec, 0.742 sec/batch)\n",
      "step 510, loss=1.47 (171.7 examples/sec, 0.746 sec/batch)\n",
      "step 520, loss=1.50 (171.8 examples/sec, 0.745 sec/batch)\n",
      "step 530, loss=1.39 (120.8 examples/sec, 1.060 sec/batch)\n",
      "step 540, loss=1.36 (190.2 examples/sec, 0.673 sec/batch)\n",
      "step 550, loss=1.47 (188.3 examples/sec, 0.680 sec/batch)\n",
      "step 560, loss=1.47 (188.9 examples/sec, 0.678 sec/batch)\n",
      "step 570, loss=1.56 (172.5 examples/sec, 0.742 sec/batch)\n",
      "step 580, loss=1.36 (172.2 examples/sec, 0.744 sec/batch)\n",
      "step 590, loss=1.34 (172.7 examples/sec, 0.741 sec/batch)\n",
      "step 600, loss=1.44 (188.4 examples/sec, 0.679 sec/batch)\n",
      "step 610, loss=1.45 (121.0 examples/sec, 1.058 sec/batch)\n",
      "step 620, loss=1.49 (189.7 examples/sec, 0.675 sec/batch)\n",
      "step 630, loss=1.53 (189.7 examples/sec, 0.675 sec/batch)\n",
      "step 640, loss=1.28 (171.8 examples/sec, 0.745 sec/batch)\n",
      "step 650, loss=1.26 (172.8 examples/sec, 0.741 sec/batch)\n",
      "step 660, loss=1.37 (170.7 examples/sec, 0.750 sec/batch)\n",
      "step 670, loss=1.45 (173.6 examples/sec, 0.737 sec/batch)\n",
      "step 680, loss=1.50 (188.5 examples/sec, 0.679 sec/batch)\n",
      "step 690, loss=1.38 (120.9 examples/sec, 1.059 sec/batch)\n",
      "step 700, loss=1.44 (190.9 examples/sec, 0.670 sec/batch)\n",
      "step 710, loss=1.40 (189.4 examples/sec, 0.676 sec/batch)\n",
      "step 720, loss=1.45 (172.9 examples/sec, 0.740 sec/batch)\n",
      "step 730, loss=1.50 (172.8 examples/sec, 0.741 sec/batch)\n",
      "step 740, loss=1.42 (172.1 examples/sec, 0.744 sec/batch)\n",
      "step 750, loss=1.31 (184.9 examples/sec, 0.692 sec/batch)\n",
      "step 760, loss=1.41 (117.1 examples/sec, 1.093 sec/batch)\n",
      "step 770, loss=1.43 (190.3 examples/sec, 0.673 sec/batch)\n",
      "step 780, loss=1.37 (190.3 examples/sec, 0.673 sec/batch)\n",
      "step 790, loss=1.28 (173.1 examples/sec, 0.739 sec/batch)\n",
      "step 800, loss=1.29 (171.6 examples/sec, 0.746 sec/batch)\n",
      "step 810, loss=1.21 (172.2 examples/sec, 0.743 sec/batch)\n",
      "step 820, loss=1.34 (181.3 examples/sec, 0.706 sec/batch)\n",
      "step 830, loss=1.33 (181.7 examples/sec, 0.705 sec/batch)\n",
      "step 840, loss=1.25 (121.8 examples/sec, 1.050 sec/batch)\n",
      "step 850, loss=1.48 (162.0 examples/sec, 0.790 sec/batch)\n",
      "step 860, loss=1.16 (148.9 examples/sec, 0.860 sec/batch)\n",
      "step 870, loss=1.35 (82.1 examples/sec, 1.559 sec/batch)\n",
      "step 880, loss=1.39 (155.9 examples/sec, 0.821 sec/batch)\n",
      "step 890, loss=1.32 (190.9 examples/sec, 0.671 sec/batch)\n",
      "step 900, loss=1.25 (93.9 examples/sec, 1.363 sec/batch)\n",
      "step 910, loss=1.34 (182.9 examples/sec, 0.700 sec/batch)\n",
      "step 920, loss=1.29 (172.5 examples/sec, 0.742 sec/batch)\n",
      "step 930, loss=1.37 (169.4 examples/sec, 0.756 sec/batch)\n",
      "step 940, loss=1.40 (173.6 examples/sec, 0.737 sec/batch)\n",
      "step 950, loss=1.36 (184.6 examples/sec, 0.693 sec/batch)\n",
      "step 960, loss=1.27 (116.6 examples/sec, 1.098 sec/batch)\n",
      "step 970, loss=1.18 (187.7 examples/sec, 0.682 sec/batch)\n",
      "step 980, loss=1.20 (96.0 examples/sec, 1.334 sec/batch)\n",
      "step 990, loss=1.27 (173.1 examples/sec, 0.739 sec/batch)\n"
     ]
    }
   ],
   "source": [
    "max_steps = 1000\n",
    "for step in range(max_steps):\n",
    "    start_time = time.time()\n",
    "    image_batch, label_batch = sess.run([images_train, labels_train])\n",
    "    _, loss_value = sess.run([train_op, loss], \n",
    "                             feed_dict={image_holder:image_batch, label_holder:label_batch})\n",
    "    duration = time.time() - start_time\n",
    "    if step % 10 ==0:\n",
    "        examples_per_sec = batch_size / duration\n",
    "        sec_per_batch = float(duration)\n",
    "        \n",
    "        format_str=('step %d, loss=%.2f (%.1f examples/sec, %.3f sec/batch)')\n",
    "        print(format_str % (step, loss_value, examples_per_sec, sec_per_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluation and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision @ top1 = 0.630\n"
     ]
    }
   ],
   "source": [
    "num_examples = 10000\n",
    "num_iter = int(math.ceil(num_examples / batch_size))\n",
    "true_count = 0\n",
    "total_sample_count = num_iter * batch_size\n",
    "step = 0\n",
    "while step < num_iter:\n",
    "    image_batch, label_batch = sess.run([images_test, labels_test])\n",
    "    # top_k_op: k=1\n",
    "    predictions = sess.run([top_k_op], feed_dict={\n",
    "        image_holder: image_batch, label_holder: label_batch})\n",
    "    true_count += np.sum(predictions)\n",
    "    step +=1\n",
    "    \n",
    "precision = true_count * 1.0 / total_sample_count\n",
    "print('precision @ top1 = %.3f' % precision)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
