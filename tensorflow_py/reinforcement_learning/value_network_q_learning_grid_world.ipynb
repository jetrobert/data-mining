{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# object class in environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GameOb():\n",
    "    def __init__(self, coordinates, size, intensity, channel, reward, name):\n",
    "        self.x = coordinates[0]\n",
    "        self.y = coordinates[1]\n",
    "        self.size = size\n",
    "        self.intensity = intensity\n",
    "        self.channel = channel\n",
    "        self.reward = reward\n",
    "        self.name = name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# grid world environment class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GameEnv():\n",
    "    def __init__(self, size):\n",
    "        self.sizeX = size\n",
    "        self.sizeY = size\n",
    "        self.actions = 4\n",
    "        self.objects = []\n",
    "        a = self.reset()\n",
    "        plt.imshow(a, interpolation=\"nearest\")\n",
    "        \n",
    "    # reset    \n",
    "    def reset(self):\n",
    "        self.objects = []\n",
    "        hero = GameOb(self.new_position(), 1, 1, 2, None, 'hero')\n",
    "        self.objects.append(hero)\n",
    "        goal = GameOb(self.new_position(), 1, 1, 1, 1, 'goal')\n",
    "        self.objects.append(goal)\n",
    "        hole = GameOb(self.new_position(), 1, 1, 0, -1, 'fire')\n",
    "        self.objects.append(hole)\n",
    "        goal2 = GameOb(self.new_position(), 1, 1, 1, 1, 'goal')\n",
    "        self.objects.append(goal2)\n",
    "        hole2 = GameOb(self.new_position(), 1, 1, 0, -1, 'fire')\n",
    "        self.objects.append(hole2)\n",
    "        goal3 = GameOb(self.new_position(), 1, 1, 1, 1, 'goal')\n",
    "        self.objects.append(goal3)\n",
    "        goal4 = GameOb(self.new_position(), 1, 1, 1, 1, 'goal')\n",
    "        self.objects.append(goal4)\n",
    "        state = self.render_env()\n",
    "        self.state = state\n",
    "        return state\n",
    "    \n",
    "    # move operation\n",
    "    def move_char(self, direction):\n",
    "        hero = self.objects[0]\n",
    "        heroX = hero.x\n",
    "        heroY = hero.y\n",
    "        if direction == 0 and hero.y >= 1:\n",
    "            hero.y -= 1\n",
    "        if direction == 1 and hero.y <= self.sizeY-2:\n",
    "            hero.y += 1\n",
    "        if direction == 2 and hero.x >= 1:\n",
    "            hero.x -= 1\n",
    "        if direction == 3 and hero.x <= self.sizeX-2:\n",
    "            hero.x += 1\n",
    "        self.objects[0] = hero\n",
    "        \n",
    "    # new position\n",
    "    def new_position(self):\n",
    "        iterables = [ range(self.sizeX), range(self.sizeY)]\n",
    "        points = []\n",
    "        for t in itertools.product(*iterables):\n",
    "            points.append(t)\n",
    "        current_positions = []\n",
    "        for objectA in self.objects:\n",
    "            if (objectA.x, objectA.y) not in current_positions:\n",
    "                current_positions.append((objectA.x, objectA.y))\n",
    "        for pos in current_positions:\n",
    "            points.remove(pos)\n",
    "        location = np.random.choice(range(len(points)), replace=False)\n",
    "        return points[location]\n",
    "    \n",
    "    # check goal\n",
    "    def check_goal(self):\n",
    "        others = []\n",
    "        for obj in self.objects:\n",
    "            if obj.name == 'hero':\n",
    "                hero = obj\n",
    "            else:\n",
    "                others.append(obj)\n",
    "        for other in others:\n",
    "            if hero.x == other.x and hero.y == other.y:\n",
    "                self.objects.remove(other)\n",
    "                if other.reward == 1:\n",
    "                    self.objects.append(GameOb(\n",
    "                        self.new_position(), 1, 1, 1, 1, 'goal'))\n",
    "                else:\n",
    "                    self.objects.append(GameOb(\n",
    "                        self.new_position(), 1, 1, 0, -1, 'fire'))\n",
    "                return other.reward, False\n",
    "        return 0.0, False\n",
    "    \n",
    "    # render environment\n",
    "    def render_env(self):\n",
    "        a = np.ones([self.sizeY+2, self.sizeX+2, 3])\n",
    "        a[1:-1, 1:-1,:] = 0\n",
    "        hero = None\n",
    "        for item in self.objects:\n",
    "            a[item.y+1:item.y+item.size+1, item.x+1:item.x+item.size+1,\n",
    "                 item.channel] = item.intensity\n",
    "        b = scipy.misc.imresize(a[:,:,0], [84,84,1], interp='nearest')\n",
    "        c = scipy.misc.imresize(a[:,:,1], [84,84,1], interp='nearest')\n",
    "        d = scipy.misc.imresize(a[:,:,2], [84,84,1], interp='nearest')\n",
    "        a = np.stack([b,c,d], axis=2)\n",
    "        return a\n",
    "    \n",
    "    # action and check\n",
    "    def step(self, action):\n",
    "        self.move_char(action)\n",
    "        reward, done = self.check_goal()\n",
    "        state = self.render_env()\n",
    "        return state, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deep q-network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self, h_size):\n",
    "        self.scalar_input = tf.placeholder(shape=[None, 21168], dtype=tf.float32)\n",
    "        self.image_in = tf.reshape(self.scalar_input, shape=[-1,84,84,3])\n",
    "        self.conv1 = tf.contrib.layers.convolution2d(\n",
    "            inputs=self.image_in, num_outputs=32,\n",
    "            kernel_size=[8,8], stride=[4,4],\n",
    "            padding='valid', biases_initializer=None)\n",
    "        self.conv2 = tf.contrib.layers.convolution2d(\n",
    "            inputs=self.conv1, num_outputs=64,\n",
    "            kernel_size=[4,4], stride=[2,2],\n",
    "            padding='valid', biases_initializer=None)\n",
    "        self.conv3 = tf.contrib.layers.convolution2d(\n",
    "            inputs=self.conv2, num_outputs=64,\n",
    "            kernel_size=[3,3], stride=[1,1],\n",
    "            padding='valid', biases_initializer=None)\n",
    "        self.conv4 = tf.contrib.layers.convolution2d(\n",
    "            inputs=self.conv3, num_outputs=512,\n",
    "            kernel_size=[7,7], stride=[1,1],\n",
    "            padding='valid', biases_initializer=None)\n",
    "        \n",
    "        # split q funtion to static (value) and dynamic (action) parts\n",
    "        self.streamAC, self.streamVC = tf.split(self.conv4, 2, 3)\n",
    "        self.streamA = tf.contrib.layers.flatten(self.streamAC)\n",
    "        self.streamV = tf.contrib.layers.flatten(self.streamVC)\n",
    "        self.aw = tf.Variable(tf.random_normal([h_size//2, env.actions]))\n",
    "        self.vw = tf.Variable(tf.random_normal([h_size//2, 1]))\n",
    "        self.advantage = tf.matmul(self.streamA, self.aw)\n",
    "        self.value = tf.matmul(self.streamV, self.vw)\n",
    "        \n",
    "        # merge value and advantage\n",
    "        self.q_out = self.value + tf.subtract(self.advantage, \n",
    "            tf.reduce_mean(self.advantage, reduction_indices=1, keep_dims=True))\n",
    "        self.predict = tf.argmax(self.q_out, 1)\n",
    "        \n",
    "        # placeholder and calculate q\n",
    "        self.targetQ = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions, env.actions, dtype=tf.float32)\n",
    "        self.q = tf.reduce_sum(tf.multiply(self.q_out, self.actions_onehot),\n",
    "                              reduction_indices=1)\n",
    "        \n",
    "        # define loss\n",
    "        self.td_error = tf.square(self.targetQ - self.q)\n",
    "        self.loss = tf.reduce_mean(self.td_error)\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.update_model = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experience replay class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size=50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "        \n",
    "    def add(self, experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer)) - self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "        \n",
    "    def sample(self, size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer, size)), [size, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# flat states function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_state(states):\n",
    "    return np.reshape(states, [21168])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# update target dqn-network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_target_graph(tf_vars, tau):\n",
    "    total_vars = len(tf_vars)\n",
    "    op_holder = []\n",
    "    for idx, var in enumerate(tf_vars[0:total_vars//2]):\n",
    "        op_holder.append(tf_vars[idx+total_vars//2].assign((var.value() * tau) + \\\n",
    "                ((1-tau) * tf_vars[idx+total_vars//2].value())))\n",
    "    return op_holder\n",
    "\n",
    "def update_target(op_holder, sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# grid world environment instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1964348b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = GameEnv(size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dqn-network and train parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sample size from experience buffer\n",
    "batch_size = 32\n",
    "# model parameters update freq\n",
    "update_freq = 4\n",
    "# q discount factor\n",
    "y = .99\n",
    "# random action probability in the beginning\n",
    "startE = 1\n",
    "endE = 0.1\n",
    "# steps from primary random prob to final random prob\n",
    "anneling_steps = 10000.\n",
    "# nums of grid world env experiment\n",
    "num_episodes = 10000\n",
    "# dqn random action test\n",
    "pre_train_steps = 10000\n",
    "# action steps per episode\n",
    "max_ep_length = 50\n",
    "load_model = False\n",
    "# model save path\n",
    "path = \"./dqn\"\n",
    "# final fully-connected layer hidden nodes\n",
    "h_size = 512\n",
    "# learning rate from target dqn to main dqn\n",
    "tau = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# init mainQN and targetQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mainQN = Qnetwork(h_size)\n",
    "targetQN = Qnetwork(h_size)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# train parameters\n",
    "trainables = tf.trainable_variables()\n",
    "target_ops = update_target_graph(trainables, tau)\n",
    "\n",
    "my_buffer = experience_buffer()\n",
    "e = startE\n",
    "step_drop = (startE - endE) / anneling_steps\n",
    "\n",
    "r_list = []\n",
    "total_steps = 0\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sess, experience buffer save, avg reward and model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('episode', 25, ', average reward of last 25 episode', 1.8799999999999999)\n",
      "('episode', 50, ', average reward of last 25 episode', 1.24)\n",
      "('episode', 75, ', average reward of last 25 episode', 1.6399999999999999)\n",
      "('episode', 100, ', average reward of last 25 episode', 1.2)\n",
      "('episode', 125, ', average reward of last 25 episode', 2.3199999999999998)\n",
      "('episode', 150, ', average reward of last 25 episode', 2.1200000000000001)\n",
      "('episode', 175, ', average reward of last 25 episode', 2.3999999999999999)\n",
      "('episode', 200, ', average reward of last 25 episode', 2.2799999999999998)\n",
      "('episode', 225, ', average reward of last 25 episode', 2.1200000000000001)\n",
      "('episode', 250, ', average reward of last 25 episode', 2.0)\n",
      "('episode', 275, ', average reward of last 25 episode', 1.8)\n",
      "('episode', 300, ', average reward of last 25 episode', 0.80000000000000004)\n",
      "('episode', 325, ', average reward of last 25 episode', 0.88)\n",
      "('episode', 350, ', average reward of last 25 episode', 1.3600000000000001)\n",
      "('episode', 375, ', average reward of last 25 episode', 0.95999999999999996)\n",
      "('episode', 400, ', average reward of last 25 episode', 0.80000000000000004)\n",
      "('episode', 425, ', average reward of last 25 episode', 0.56000000000000005)\n",
      "('episode', 450, ', average reward of last 25 episode', 0.92000000000000004)\n",
      "('episode', 475, ', average reward of last 25 episode', 0.92000000000000004)\n",
      "('episode', 500, ', average reward of last 25 episode', 0.40000000000000002)\n",
      "('episode', 525, ', average reward of last 25 episode', 1.6000000000000001)\n",
      "('episode', 550, ', average reward of last 25 episode', 1.2)\n",
      "('episode', 575, ', average reward of last 25 episode', 1.3600000000000001)\n",
      "('episode', 600, ', average reward of last 25 episode', 1.3600000000000001)\n",
      "('episode', 625, ', average reward of last 25 episode', 0.59999999999999998)\n",
      "('episode', 650, ', average reward of last 25 episode', 1.24)\n",
      "('episode', 675, ', average reward of last 25 episode', 0.92000000000000004)\n",
      "('episode', 700, ', average reward of last 25 episode', 1.0)\n",
      "('episode', 725, ', average reward of last 25 episode', 1.24)\n",
      "('episode', 750, ', average reward of last 25 episode', 1.3999999999999999)\n",
      "('episode', 775, ', average reward of last 25 episode', 1.04)\n",
      "('episode', 800, ', average reward of last 25 episode', 1.24)\n",
      "('episode', 825, ', average reward of last 25 episode', 0.88)\n",
      "('episode', 850, ', average reward of last 25 episode', 1.48)\n",
      "('episode', 875, ', average reward of last 25 episode', 0.76000000000000001)\n",
      "('episode', 900, ', average reward of last 25 episode', 2.04)\n",
      "('episode', 925, ', average reward of last 25 episode', 1.6399999999999999)\n",
      "('episode', 950, ', average reward of last 25 episode', 1.52)\n",
      "('episode', 975, ', average reward of last 25 episode', 1.48)\n",
      "('episode', 1000, ', average reward of last 25 episode', 1.3999999999999999)\n",
      "Saved Model!\n",
      "('episode', 1025, ', average reward of last 25 episode', 2.04)\n",
      "('episode', 1050, ', average reward of last 25 episode', 2.0800000000000001)\n",
      "('episode', 1075, ', average reward of last 25 episode', 2.0)\n",
      "('episode', 1100, ', average reward of last 25 episode', 1.28)\n",
      "('episode', 1125, ', average reward of last 25 episode', 2.3999999999999999)\n",
      "('episode', 1150, ', average reward of last 25 episode', 1.24)\n",
      "('episode', 1175, ', average reward of last 25 episode', 1.72)\n",
      "('episode', 1200, ', average reward of last 25 episode', 1.2)\n",
      "('episode', 1225, ', average reward of last 25 episode', 1.4399999999999999)\n",
      "('episode', 1250, ', average reward of last 25 episode', 1.04)\n",
      "('episode', 1275, ', average reward of last 25 episode', 1.6799999999999999)\n",
      "('episode', 1300, ', average reward of last 25 episode', 2.3199999999999998)\n",
      "('episode', 1325, ', average reward of last 25 episode', 3.48)\n",
      "('episode', 1350, ', average reward of last 25 episode', 2.5600000000000001)\n",
      "('episode', 1375, ', average reward of last 25 episode', 1.96)\n",
      "('episode', 1400, ', average reward of last 25 episode', 2.6400000000000001)\n",
      "('episode', 1425, ', average reward of last 25 episode', 3.4399999999999999)\n",
      "('episode', 1450, ', average reward of last 25 episode', 2.96)\n",
      "('episode', 1475, ', average reward of last 25 episode', 2.6000000000000001)\n",
      "('episode', 1500, ', average reward of last 25 episode', 3.6400000000000001)\n",
      "('episode', 1525, ', average reward of last 25 episode', 2.04)\n",
      "('episode', 1550, ', average reward of last 25 episode', 4.2800000000000002)\n",
      "('episode', 1575, ', average reward of last 25 episode', 3.0800000000000001)\n",
      "('episode', 1600, ', average reward of last 25 episode', 3.3599999999999999)\n",
      "('episode', 1625, ', average reward of last 25 episode', 3.7599999999999998)\n",
      "('episode', 1650, ', average reward of last 25 episode', 4.8799999999999999)\n",
      "('episode', 1675, ', average reward of last 25 episode', 3.48)\n",
      "('episode', 1700, ', average reward of last 25 episode', 5.8399999999999999)\n",
      "('episode', 1725, ', average reward of last 25 episode', 5.2800000000000002)\n",
      "('episode', 1750, ', average reward of last 25 episode', 4.8399999999999999)\n",
      "('episode', 1775, ', average reward of last 25 episode', 5.3200000000000003)\n",
      "('episode', 1800, ', average reward of last 25 episode', 5.4400000000000004)\n",
      "('episode', 1825, ', average reward of last 25 episode', 6.4800000000000004)\n",
      "('episode', 1850, ', average reward of last 25 episode', 8.5999999999999996)\n",
      "('episode', 1875, ', average reward of last 25 episode', 7.5999999999999996)\n",
      "('episode', 1900, ', average reward of last 25 episode', 8.6400000000000006)\n",
      "('episode', 1925, ', average reward of last 25 episode', 9.5999999999999996)\n",
      "('episode', 1950, ', average reward of last 25 episode', 10.44)\n",
      "('episode', 1975, ', average reward of last 25 episode', 9.8000000000000007)\n",
      "('episode', 2000, ', average reward of last 25 episode', 9.7200000000000006)\n",
      "Saved Model!\n",
      "('episode', 2025, ', average reward of last 25 episode', 12.24)\n",
      "('episode', 2050, ', average reward of last 25 episode', 12.68)\n",
      "('episode', 2075, ', average reward of last 25 episode', 11.52)\n",
      "('episode', 2100, ', average reward of last 25 episode', 11.92)\n",
      "('episode', 2125, ', average reward of last 25 episode', 14.48)\n",
      "('episode', 2150, ', average reward of last 25 episode', 15.44)\n",
      "('episode', 2175, ', average reward of last 25 episode', 14.279999999999999)\n",
      "('episode', 2200, ', average reward of last 25 episode', 13.6)\n",
      "('episode', 2225, ', average reward of last 25 episode', 15.800000000000001)\n",
      "('episode', 2250, ', average reward of last 25 episode', 15.199999999999999)\n",
      "('episode', 2275, ', average reward of last 25 episode', 13.48)\n",
      "('episode', 2300, ', average reward of last 25 episode', 15.800000000000001)\n",
      "('episode', 2325, ', average reward of last 25 episode', 14.92)\n",
      "('episode', 2350, ', average reward of last 25 episode', 16.760000000000002)\n",
      "('episode', 2375, ', average reward of last 25 episode', 16.640000000000001)\n",
      "('episode', 2400, ', average reward of last 25 episode', 17.399999999999999)\n",
      "('episode', 2425, ', average reward of last 25 episode', 17.600000000000001)\n",
      "('episode', 2450, ', average reward of last 25 episode', 16.199999999999999)\n",
      "('episode', 2475, ', average reward of last 25 episode', 17.68)\n",
      "('episode', 2500, ', average reward of last 25 episode', 18.920000000000002)\n",
      "('episode', 2525, ', average reward of last 25 episode', 18.559999999999999)\n",
      "('episode', 2550, ', average reward of last 25 episode', 19.16)\n",
      "('episode', 2575, ', average reward of last 25 episode', 19.399999999999999)\n",
      "('episode', 2600, ', average reward of last 25 episode', 17.239999999999998)\n",
      "('episode', 2625, ', average reward of last 25 episode', 16.879999999999999)\n",
      "('episode', 2650, ', average reward of last 25 episode', 19.16)\n",
      "('episode', 2675, ', average reward of last 25 episode', 18.879999999999999)\n",
      "('episode', 2700, ', average reward of last 25 episode', 19.719999999999999)\n",
      "('episode', 2725, ', average reward of last 25 episode', 19.399999999999999)\n",
      "('episode', 2750, ', average reward of last 25 episode', 19.719999999999999)\n",
      "('episode', 2775, ', average reward of last 25 episode', 19.16)\n",
      "('episode', 2800, ', average reward of last 25 episode', 20.16)\n",
      "('episode', 2825, ', average reward of last 25 episode', 20.559999999999999)\n",
      "('episode', 2850, ', average reward of last 25 episode', 20.719999999999999)\n",
      "('episode', 2875, ', average reward of last 25 episode', 21.120000000000001)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('episode', 2900, ', average reward of last 25 episode', 19.68)\n",
      "('episode', 2925, ', average reward of last 25 episode', 19.640000000000001)\n",
      "('episode', 2950, ', average reward of last 25 episode', 18.920000000000002)\n",
      "('episode', 2975, ', average reward of last 25 episode', 19.760000000000002)\n",
      "('episode', 3000, ', average reward of last 25 episode', 20.52)\n",
      "Saved Model!\n",
      "('episode', 3025, ', average reward of last 25 episode', 20.68)\n",
      "('episode', 3050, ', average reward of last 25 episode', 21.32)\n",
      "('episode', 3075, ', average reward of last 25 episode', 20.559999999999999)\n",
      "('episode', 3100, ', average reward of last 25 episode', 20.440000000000001)\n",
      "('episode', 3125, ', average reward of last 25 episode', 20.399999999999999)\n",
      "('episode', 3150, ', average reward of last 25 episode', 21.120000000000001)\n",
      "('episode', 3175, ', average reward of last 25 episode', 21.199999999999999)\n",
      "('episode', 3200, ', average reward of last 25 episode', 20.16)\n",
      "('episode', 3225, ', average reward of last 25 episode', 21.52)\n",
      "('episode', 3250, ', average reward of last 25 episode', 20.559999999999999)\n",
      "('episode', 3275, ', average reward of last 25 episode', 20.32)\n",
      "('episode', 3300, ', average reward of last 25 episode', 21.120000000000001)\n",
      "('episode', 3325, ', average reward of last 25 episode', 21.199999999999999)\n",
      "('episode', 3350, ', average reward of last 25 episode', 21.760000000000002)\n",
      "('episode', 3375, ', average reward of last 25 episode', 21.359999999999999)\n",
      "('episode', 3400, ', average reward of last 25 episode', 20.920000000000002)\n",
      "('episode', 3425, ', average reward of last 25 episode', 20.399999999999999)\n",
      "('episode', 3450, ', average reward of last 25 episode', 20.800000000000001)\n",
      "('episode', 3475, ', average reward of last 25 episode', 21.120000000000001)\n",
      "('episode', 3500, ', average reward of last 25 episode', 21.84)\n",
      "('episode', 3525, ', average reward of last 25 episode', 21.800000000000001)\n",
      "('episode', 3550, ', average reward of last 25 episode', 21.440000000000001)\n",
      "('episode', 3575, ', average reward of last 25 episode', 20.960000000000001)\n",
      "('episode', 3600, ', average reward of last 25 episode', 21.280000000000001)\n",
      "('episode', 3625, ', average reward of last 25 episode', 22.239999999999998)\n",
      "('episode', 3650, ', average reward of last 25 episode', 20.52)\n",
      "('episode', 3675, ', average reward of last 25 episode', 22.32)\n",
      "('episode', 3700, ', average reward of last 25 episode', 21.68)\n",
      "('episode', 3725, ', average reward of last 25 episode', 22.440000000000001)\n",
      "('episode', 3750, ', average reward of last 25 episode', 21.239999999999998)\n",
      "('episode', 3775, ', average reward of last 25 episode', 22.280000000000001)\n",
      "('episode', 3800, ', average reward of last 25 episode', 21.440000000000001)\n",
      "('episode', 3825, ', average reward of last 25 episode', 22.640000000000001)\n",
      "('episode', 3850, ', average reward of last 25 episode', 21.760000000000002)\n",
      "('episode', 3875, ', average reward of last 25 episode', 21.640000000000001)\n",
      "('episode', 3900, ', average reward of last 25 episode', 21.920000000000002)\n",
      "('episode', 3925, ', average reward of last 25 episode', 22.359999999999999)\n",
      "('episode', 3950, ', average reward of last 25 episode', 20.920000000000002)\n",
      "('episode', 3975, ', average reward of last 25 episode', 21.600000000000001)\n",
      "('episode', 4000, ', average reward of last 25 episode', 22.440000000000001)\n",
      "Saved Model!\n",
      "('episode', 4025, ', average reward of last 25 episode', 21.32)\n",
      "('episode', 4050, ', average reward of last 25 episode', 22.719999999999999)\n",
      "('episode', 4075, ', average reward of last 25 episode', 21.120000000000001)\n",
      "('episode', 4100, ', average reward of last 25 episode', 19.440000000000001)\n",
      "('episode', 4125, ', average reward of last 25 episode', 21.079999999999998)\n",
      "('episode', 4150, ', average reward of last 25 episode', 22.559999999999999)\n",
      "('episode', 4175, ', average reward of last 25 episode', 20.920000000000002)\n",
      "('episode', 4200, ', average reward of last 25 episode', 22.640000000000001)\n",
      "('episode', 4225, ', average reward of last 25 episode', 21.719999999999999)\n",
      "('episode', 4250, ', average reward of last 25 episode', 21.48)\n",
      "('episode', 4275, ', average reward of last 25 episode', 21.640000000000001)\n",
      "('episode', 4300, ', average reward of last 25 episode', 20.52)\n",
      "('episode', 4325, ', average reward of last 25 episode', 21.120000000000001)\n",
      "('episode', 4350, ', average reward of last 25 episode', 21.48)\n",
      "('episode', 4375, ', average reward of last 25 episode', 22.760000000000002)\n",
      "('episode', 4400, ', average reward of last 25 episode', 21.48)\n",
      "('episode', 4425, ', average reward of last 25 episode', 22.760000000000002)\n",
      "('episode', 4450, ', average reward of last 25 episode', 22.719999999999999)\n",
      "('episode', 4475, ', average reward of last 25 episode', 20.800000000000001)\n",
      "('episode', 4500, ', average reward of last 25 episode', 21.559999999999999)\n",
      "('episode', 4525, ', average reward of last 25 episode', 23.039999999999999)\n",
      "('episode', 4550, ', average reward of last 25 episode', 23.199999999999999)\n",
      "('episode', 4575, ', average reward of last 25 episode', 23.239999999999998)\n",
      "('episode', 4600, ', average reward of last 25 episode', 22.920000000000002)\n",
      "('episode', 4625, ', average reward of last 25 episode', 22.559999999999999)\n",
      "('episode', 4650, ', average reward of last 25 episode', 21.640000000000001)\n",
      "('episode', 4675, ', average reward of last 25 episode', 22.079999999999998)\n",
      "('episode', 4700, ', average reward of last 25 episode', 22.239999999999998)\n",
      "('episode', 4725, ', average reward of last 25 episode', 22.719999999999999)\n",
      "('episode', 4750, ', average reward of last 25 episode', 20.600000000000001)\n",
      "('episode', 4775, ', average reward of last 25 episode', 22.120000000000001)\n",
      "('episode', 4800, ', average reward of last 25 episode', 22.440000000000001)\n",
      "('episode', 4825, ', average reward of last 25 episode', 22.440000000000001)\n",
      "('episode', 4850, ', average reward of last 25 episode', 21.640000000000001)\n",
      "('episode', 4875, ', average reward of last 25 episode', 22.640000000000001)\n",
      "('episode', 4900, ', average reward of last 25 episode', 23.079999999999998)\n",
      "('episode', 4925, ', average reward of last 25 episode', 20.84)\n",
      "('episode', 4950, ', average reward of last 25 episode', 21.039999999999999)\n",
      "('episode', 4975, ', average reward of last 25 episode', 23.32)\n",
      "('episode', 5000, ', average reward of last 25 episode', 22.879999999999999)\n",
      "Saved Model!\n",
      "('episode', 5025, ', average reward of last 25 episode', 21.16)\n",
      "('episode', 5050, ', average reward of last 25 episode', 22.879999999999999)\n",
      "('episode', 5075, ', average reward of last 25 episode', 21.640000000000001)\n",
      "('episode', 5100, ', average reward of last 25 episode', 21.84)\n",
      "('episode', 5125, ', average reward of last 25 episode', 20.760000000000002)\n",
      "('episode', 5150, ', average reward of last 25 episode', 21.960000000000001)\n",
      "('episode', 5175, ', average reward of last 25 episode', 22.68)\n",
      "('episode', 5200, ', average reward of last 25 episode', 21.920000000000002)\n",
      "('episode', 5225, ', average reward of last 25 episode', 21.399999999999999)\n",
      "('episode', 5250, ', average reward of last 25 episode', 22.719999999999999)\n",
      "('episode', 5275, ', average reward of last 25 episode', 22.440000000000001)\n",
      "('episode', 5300, ', average reward of last 25 episode', 22.600000000000001)\n",
      "('episode', 5325, ', average reward of last 25 episode', 21.879999999999999)\n",
      "('episode', 5350, ', average reward of last 25 episode', 22.039999999999999)\n",
      "('episode', 5375, ', average reward of last 25 episode', 22.84)\n",
      "('episode', 5400, ', average reward of last 25 episode', 21.32)\n",
      "('episode', 5425, ', average reward of last 25 episode', 22.039999999999999)\n",
      "('episode', 5450, ', average reward of last 25 episode', 21.32)\n",
      "('episode', 5475, ', average reward of last 25 episode', 22.079999999999998)\n",
      "('episode', 5500, ', average reward of last 25 episode', 20.719999999999999)\n",
      "('episode', 5525, ', average reward of last 25 episode', 22.120000000000001)\n",
      "('episode', 5550, ', average reward of last 25 episode', 22.0)\n",
      "('episode', 5575, ', average reward of last 25 episode', 22.239999999999998)\n",
      "('episode', 5600, ', average reward of last 25 episode', 22.120000000000001)\n",
      "('episode', 5625, ', average reward of last 25 episode', 22.039999999999999)\n",
      "('episode', 5650, ', average reward of last 25 episode', 22.039999999999999)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('episode', 5675, ', average reward of last 25 episode', 21.960000000000001)\n",
      "('episode', 5700, ', average reward of last 25 episode', 23.239999999999998)\n",
      "('episode', 5725, ', average reward of last 25 episode', 21.640000000000001)\n",
      "('episode', 5750, ', average reward of last 25 episode', 22.039999999999999)\n",
      "('episode', 5775, ', average reward of last 25 episode', 23.0)\n",
      "('episode', 5800, ', average reward of last 25 episode', 21.760000000000002)\n",
      "('episode', 5825, ', average reward of last 25 episode', 19.879999999999999)\n",
      "('episode', 5850, ', average reward of last 25 episode', 22.48)\n",
      "('episode', 5875, ', average reward of last 25 episode', 23.079999999999998)\n",
      "('episode', 5900, ', average reward of last 25 episode', 21.600000000000001)\n",
      "('episode', 5925, ', average reward of last 25 episode', 22.52)\n",
      "('episode', 5950, ', average reward of last 25 episode', 22.960000000000001)\n",
      "('episode', 5975, ', average reward of last 25 episode', 21.359999999999999)\n",
      "('episode', 6000, ', average reward of last 25 episode', 22.32)\n",
      "Saved Model!\n",
      "('episode', 6025, ', average reward of last 25 episode', 21.879999999999999)\n",
      "('episode', 6050, ', average reward of last 25 episode', 21.960000000000001)\n",
      "('episode', 6075, ', average reward of last 25 episode', 21.600000000000001)\n",
      "('episode', 6100, ', average reward of last 25 episode', 22.559999999999999)\n",
      "('episode', 6125, ', average reward of last 25 episode', 22.68)\n",
      "('episode', 6150, ', average reward of last 25 episode', 21.960000000000001)\n",
      "('episode', 6175, ', average reward of last 25 episode', 22.52)\n",
      "('episode', 6200, ', average reward of last 25 episode', 21.84)\n",
      "('episode', 6225, ', average reward of last 25 episode', 22.920000000000002)\n",
      "('episode', 6250, ', average reward of last 25 episode', 22.440000000000001)\n",
      "('episode', 6275, ', average reward of last 25 episode', 23.079999999999998)\n",
      "('episode', 6300, ', average reward of last 25 episode', 22.68)\n",
      "('episode', 6325, ', average reward of last 25 episode', 21.280000000000001)\n",
      "('episode', 6350, ', average reward of last 25 episode', 24.239999999999998)\n",
      "('episode', 6375, ', average reward of last 25 episode', 22.399999999999999)\n",
      "('episode', 6400, ', average reward of last 25 episode', 23.079999999999998)\n",
      "('episode', 6425, ', average reward of last 25 episode', 22.239999999999998)\n",
      "('episode', 6450, ', average reward of last 25 episode', 21.719999999999999)\n",
      "('episode', 6475, ', average reward of last 25 episode', 22.84)\n",
      "('episode', 6500, ', average reward of last 25 episode', 21.960000000000001)\n",
      "('episode', 6525, ', average reward of last 25 episode', 23.079999999999998)\n",
      "('episode', 6550, ', average reward of last 25 episode', 21.640000000000001)\n",
      "('episode', 6575, ', average reward of last 25 episode', 21.600000000000001)\n",
      "('episode', 6600, ', average reward of last 25 episode', 22.600000000000001)\n",
      "('episode', 6625, ', average reward of last 25 episode', 22.800000000000001)\n",
      "('episode', 6650, ', average reward of last 25 episode', 23.120000000000001)\n",
      "('episode', 6675, ', average reward of last 25 episode', 22.120000000000001)\n",
      "('episode', 6700, ', average reward of last 25 episode', 22.760000000000002)\n",
      "('episode', 6725, ', average reward of last 25 episode', 21.120000000000001)\n",
      "('episode', 6750, ', average reward of last 25 episode', 22.920000000000002)\n",
      "('episode', 6775, ', average reward of last 25 episode', 22.52)\n",
      "('episode', 6800, ', average reward of last 25 episode', 23.84)\n",
      "('episode', 6825, ', average reward of last 25 episode', 21.440000000000001)\n",
      "('episode', 6850, ', average reward of last 25 episode', 22.640000000000001)\n",
      "('episode', 6875, ', average reward of last 25 episode', 21.719999999999999)\n",
      "('episode', 6900, ', average reward of last 25 episode', 22.120000000000001)\n",
      "('episode', 6925, ', average reward of last 25 episode', 22.48)\n",
      "('episode', 6950, ', average reward of last 25 episode', 22.199999999999999)\n",
      "('episode', 6975, ', average reward of last 25 episode', 20.84)\n",
      "('episode', 7000, ', average reward of last 25 episode', 22.719999999999999)\n",
      "Saved Model!\n",
      "('episode', 7025, ', average reward of last 25 episode', 21.440000000000001)\n",
      "('episode', 7050, ', average reward of last 25 episode', 22.960000000000001)\n",
      "('episode', 7075, ', average reward of last 25 episode', 24.280000000000001)\n",
      "('episode', 7100, ', average reward of last 25 episode', 22.719999999999999)\n",
      "('episode', 7125, ', average reward of last 25 episode', 22.199999999999999)\n",
      "('episode', 7150, ', average reward of last 25 episode', 22.52)\n",
      "('episode', 7175, ', average reward of last 25 episode', 22.719999999999999)\n",
      "('episode', 7200, ', average reward of last 25 episode', 23.079999999999998)\n",
      "('episode', 7225, ', average reward of last 25 episode', 22.800000000000001)\n",
      "('episode', 7250, ', average reward of last 25 episode', 22.199999999999999)\n",
      "('episode', 7275, ', average reward of last 25 episode', 21.280000000000001)\n",
      "('episode', 7300, ', average reward of last 25 episode', 22.199999999999999)\n",
      "('episode', 7325, ', average reward of last 25 episode', 22.920000000000002)\n",
      "('episode', 7350, ', average reward of last 25 episode', 23.039999999999999)\n",
      "('episode', 7375, ', average reward of last 25 episode', 22.760000000000002)\n",
      "('episode', 7400, ', average reward of last 25 episode', 22.84)\n",
      "('episode', 7425, ', average reward of last 25 episode', 22.559999999999999)\n",
      "('episode', 7450, ', average reward of last 25 episode', 21.920000000000002)\n",
      "('episode', 7475, ', average reward of last 25 episode', 21.920000000000002)\n",
      "('episode', 7500, ', average reward of last 25 episode', 22.600000000000001)\n",
      "('episode', 7525, ', average reward of last 25 episode', 23.32)\n",
      "('episode', 7550, ', average reward of last 25 episode', 22.199999999999999)\n",
      "('episode', 7575, ', average reward of last 25 episode', 22.039999999999999)\n",
      "('episode', 7600, ', average reward of last 25 episode', 21.800000000000001)\n",
      "('episode', 7625, ', average reward of last 25 episode', 22.079999999999998)\n",
      "('episode', 7650, ', average reward of last 25 episode', 22.920000000000002)\n",
      "('episode', 7675, ', average reward of last 25 episode', 23.640000000000001)\n",
      "('episode', 7700, ', average reward of last 25 episode', 22.920000000000002)\n",
      "('episode', 7725, ', average reward of last 25 episode', 23.800000000000001)\n",
      "('episode', 7750, ', average reward of last 25 episode', 22.359999999999999)\n",
      "('episode', 7775, ', average reward of last 25 episode', 20.879999999999999)\n",
      "('episode', 7800, ', average reward of last 25 episode', 23.399999999999999)\n",
      "('episode', 7825, ', average reward of last 25 episode', 23.079999999999998)\n",
      "('episode', 7850, ', average reward of last 25 episode', 24.079999999999998)\n",
      "('episode', 7875, ', average reward of last 25 episode', 23.48)\n",
      "('episode', 7900, ', average reward of last 25 episode', 22.559999999999999)\n",
      "('episode', 7925, ', average reward of last 25 episode', 23.239999999999998)\n",
      "('episode', 7950, ', average reward of last 25 episode', 20.68)\n",
      "('episode', 7975, ', average reward of last 25 episode', 21.960000000000001)\n",
      "('episode', 8000, ', average reward of last 25 episode', 22.960000000000001)\n",
      "Saved Model!\n",
      "('episode', 8025, ', average reward of last 25 episode', 23.719999999999999)\n",
      "('episode', 8050, ', average reward of last 25 episode', 22.280000000000001)\n",
      "('episode', 8075, ', average reward of last 25 episode', 23.199999999999999)\n",
      "('episode', 8100, ', average reward of last 25 episode', 21.960000000000001)\n",
      "('episode', 8125, ', average reward of last 25 episode', 24.440000000000001)\n",
      "('episode', 8150, ', average reward of last 25 episode', 20.879999999999999)\n",
      "('episode', 8175, ', average reward of last 25 episode', 23.239999999999998)\n",
      "('episode', 8200, ', average reward of last 25 episode', 21.48)\n",
      "('episode', 8225, ', average reward of last 25 episode', 23.199999999999999)\n",
      "('episode', 8250, ', average reward of last 25 episode', 23.120000000000001)\n",
      "('episode', 8275, ', average reward of last 25 episode', 22.399999999999999)\n",
      "('episode', 8300, ', average reward of last 25 episode', 22.600000000000001)\n",
      "('episode', 8325, ', average reward of last 25 episode', 22.719999999999999)\n",
      "('episode', 8350, ', average reward of last 25 episode', 21.879999999999999)\n",
      "('episode', 8375, ', average reward of last 25 episode', 23.280000000000001)\n",
      "('episode', 8400, ', average reward of last 25 episode', 23.960000000000001)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('episode', 8425, ', average reward of last 25 episode', 22.359999999999999)\n",
      "('episode', 8450, ', average reward of last 25 episode', 23.079999999999998)\n",
      "('episode', 8475, ', average reward of last 25 episode', 22.84)\n",
      "('episode', 8500, ', average reward of last 25 episode', 22.32)\n",
      "('episode', 8525, ', average reward of last 25 episode', 22.0)\n",
      "('episode', 8550, ', average reward of last 25 episode', 23.359999999999999)\n",
      "('episode', 8575, ', average reward of last 25 episode', 23.48)\n",
      "('episode', 8600, ', average reward of last 25 episode', 23.800000000000001)\n",
      "('episode', 8625, ', average reward of last 25 episode', 23.16)\n",
      "('episode', 8650, ', average reward of last 25 episode', 22.16)\n",
      "('episode', 8675, ', average reward of last 25 episode', 23.800000000000001)\n",
      "('episode', 8700, ', average reward of last 25 episode', 22.879999999999999)\n",
      "('episode', 8725, ', average reward of last 25 episode', 22.359999999999999)\n",
      "('episode', 8750, ', average reward of last 25 episode', 22.199999999999999)\n",
      "('episode', 8775, ', average reward of last 25 episode', 23.280000000000001)\n",
      "('episode', 8800, ', average reward of last 25 episode', 23.600000000000001)\n",
      "('episode', 8825, ', average reward of last 25 episode', 24.239999999999998)\n",
      "('episode', 8850, ', average reward of last 25 episode', 23.719999999999999)\n",
      "('episode', 8875, ', average reward of last 25 episode', 21.399999999999999)\n",
      "('episode', 8900, ', average reward of last 25 episode', 20.84)\n",
      "('episode', 8925, ', average reward of last 25 episode', 23.68)\n",
      "('episode', 8950, ', average reward of last 25 episode', 22.559999999999999)\n",
      "('episode', 8975, ', average reward of last 25 episode', 23.32)\n",
      "('episode', 9000, ', average reward of last 25 episode', 21.879999999999999)\n",
      "Saved Model!\n",
      "('episode', 9025, ', average reward of last 25 episode', 24.239999999999998)\n",
      "('episode', 9050, ', average reward of last 25 episode', 22.879999999999999)\n",
      "('episode', 9075, ', average reward of last 25 episode', 21.920000000000002)\n",
      "('episode', 9100, ', average reward of last 25 episode', 22.32)\n",
      "('episode', 9125, ', average reward of last 25 episode', 23.280000000000001)\n",
      "('episode', 9150, ', average reward of last 25 episode', 22.399999999999999)\n",
      "('episode', 9175, ', average reward of last 25 episode', 23.440000000000001)\n",
      "('episode', 9200, ', average reward of last 25 episode', 22.68)\n",
      "('episode', 9225, ', average reward of last 25 episode', 23.640000000000001)\n",
      "('episode', 9250, ', average reward of last 25 episode', 23.719999999999999)\n",
      "('episode', 9275, ', average reward of last 25 episode', 22.719999999999999)\n",
      "('episode', 9300, ', average reward of last 25 episode', 23.120000000000001)\n",
      "('episode', 9325, ', average reward of last 25 episode', 22.68)\n",
      "('episode', 9350, ', average reward of last 25 episode', 20.68)\n",
      "('episode', 9375, ', average reward of last 25 episode', 21.760000000000002)\n",
      "('episode', 9400, ', average reward of last 25 episode', 22.239999999999998)\n",
      "('episode', 9425, ', average reward of last 25 episode', 22.920000000000002)\n",
      "('episode', 9450, ', average reward of last 25 episode', 23.280000000000001)\n",
      "('episode', 9475, ', average reward of last 25 episode', 21.760000000000002)\n",
      "('episode', 9500, ', average reward of last 25 episode', 22.84)\n",
      "('episode', 9525, ', average reward of last 25 episode', 23.359999999999999)\n",
      "('episode', 9550, ', average reward of last 25 episode', 22.640000000000001)\n",
      "('episode', 9575, ', average reward of last 25 episode', 22.640000000000001)\n",
      "('episode', 9600, ', average reward of last 25 episode', 23.359999999999999)\n",
      "('episode', 9625, ', average reward of last 25 episode', 24.440000000000001)\n",
      "('episode', 9650, ', average reward of last 25 episode', 23.199999999999999)\n",
      "('episode', 9675, ', average reward of last 25 episode', 22.920000000000002)\n",
      "('episode', 9700, ', average reward of last 25 episode', 23.800000000000001)\n",
      "('episode', 9725, ', average reward of last 25 episode', 23.199999999999999)\n",
      "('episode', 9750, ', average reward of last 25 episode', 22.32)\n",
      "('episode', 9775, ', average reward of last 25 episode', 21.719999999999999)\n",
      "('episode', 9800, ', average reward of last 25 episode', 22.800000000000001)\n",
      "('episode', 9825, ', average reward of last 25 episode', 23.52)\n",
      "('episode', 9850, ', average reward of last 25 episode', 22.84)\n",
      "('episode', 9875, ', average reward of last 25 episode', 22.920000000000002)\n",
      "('episode', 9900, ', average reward of last 25 episode', 22.399999999999999)\n",
      "('episode', 9925, ', average reward of last 25 episode', 22.399999999999999)\n",
      "('episode', 9950, ', average reward of last 25 episode', 23.079999999999998)\n",
      "('episode', 9975, ', average reward of last 25 episode', 21.920000000000002)\n",
      "('episode', 10000, ', average reward of last 25 episode', 22.48)\n",
      "Saved Model!\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    if load_model == True:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    sess.run(init)\n",
    "    update_target(target_ops, sess)\n",
    "    for i in range(num_episodes+1):\n",
    "        episode_buffer = experience_buffer()\n",
    "        s = env.reset()\n",
    "        s = process_state(s)\n",
    "        # done\n",
    "        d = False\n",
    "        # reward in episode\n",
    "        r_all = 0\n",
    "        # steps in episode\n",
    "        j = 0\n",
    "        \n",
    "        while j < max_ep_length:\n",
    "            j += 1\n",
    "            if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                a = np.random.randint(0, 4)\n",
    "            else:\n",
    "                a = sess.run(mainQN.predict, feed_dict={mainQN.scalar_input:[s]})[0]\n",
    "            s1, r, d = env.step(a)\n",
    "            s1 = process_state(s1)\n",
    "            total_steps += 1\n",
    "            episode_buffer.add(np.reshape(np.array([s,a,r,s1,d]), [1,5]))\n",
    "            \n",
    "            if total_steps > pre_train_steps:\n",
    "                if e > endE:\n",
    "                    e -= step_drop\n",
    "                if total_steps % (update_freq) == 0:\n",
    "                    train_batch = my_buffer.sample(batch_size)\n",
    "                    # input next state into mainQN and predict\n",
    "                    a = sess.run(mainQN.predict, feed_dict={\n",
    "                        mainQN.scalar_input: np.vstack(train_batch[:,3])})\n",
    "                    # then input next state into targetQN and get q\n",
    "                    q = sess.run(targetQN.q_out, feed_dict={\n",
    "                        targetQN.scalar_input: np.vstack(train_batch[:,3])})\n",
    "                    doubleQ = q[range(batch_size), a]\n",
    "                    # reward + doubleQ * discount\n",
    "                    targetQ = train_batch[:,2] + y*doubleQ\n",
    "                    _ = sess.run(mainQN.update_model, feed_dict={\n",
    "                        mainQN.scalar_input: np.vstack(train_batch[:,0]),\n",
    "                        mainQN.targetQ: targetQ,\n",
    "                        mainQN.actions: train_batch[:,1]})\n",
    "                    \n",
    "                    update_target(target_ops, sess)\n",
    "            r_all += r\n",
    "            s = s1\n",
    "            \n",
    "            if d == True:\n",
    "                break\n",
    "        \n",
    "        # model save\n",
    "        my_buffer.add(episode_buffer.buffer)\n",
    "        r_list.append(r_all)\n",
    "        if i>0 and i%25 == 0:\n",
    "            print('episode', i, ', average reward of last 25 episode', \n",
    "                  np.mean(r_list[-25:]))\n",
    "        if i>0 and i%1000 == 0:\n",
    "            saver.save(sess, path+'/model-'+str(i)+'.ckpt')\n",
    "            print(\"Saved Model!\")\n",
    "    saver.save(sess, path+'/model-'+str(i)+'.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plt reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f18d4a5a910>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/bZB36ugODoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f18d4748350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r_mat = np.resize(np.array(r_list),  [len(r_list)//100, 100])\n",
    "r_mean = np.average(r_mat, 1)\n",
    "plt.plot(r_mean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
